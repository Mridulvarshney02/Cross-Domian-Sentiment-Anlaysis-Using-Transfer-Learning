# -*- coding: utf-8 -*-
"""cross domain sentiment analysis transfer learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LMsXuk7SfLzflVcINJmWpJaHksLnTIsV
"""

! pip install transformers datasets scikit-learn

# Import required libraries
import pandas as pd
from datasets import Dataset, DatasetDict
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Load the datasets
def load_and_prepare_data():
    # Source domain dataset (IMDb Movie Reviews)
    imdb_data = pd.read_csv("/content/IMDB Dataset.csv")
    imdb_data.rename(columns={"review": "text", "sentiment": "label"}, inplace=True)
    imdb_data["label"] = imdb_data["label"].apply(lambda x: 1 if x == "positive" else 0)

    # Target domain dataset (TripAdvisor Hotel Reviews)
    hotel_data = pd.read_csv('/content/tripadvisor_hotel_reviews.csv')  # Update with the path to your local file
    hotel_data = hotel_data[["Review", "Rating"]].dropna()
    hotel_data.rename(columns={"Review": "text", "Rating": "label"}, inplace=True)

    # Convert ratings to binary sentiment: Positive = 4 or 5, Negative = 1, 2, or 3
    hotel_data["label"] = hotel_data["label"].apply(lambda x: 1 if x >= 4 else 0)

    # Wrap data into Hugging Face's Dataset format
    imdb_dataset = Dataset.from_pandas(imdb_data.sample(2000))  # Use a subset for quick training
    hotel_dataset = Dataset.from_pandas(hotel_data.sample(500))  # Limited labeled target data

    return DatasetDict({"source_train": imdb_dataset, "target_train": hotel_dataset})

# Tokenizer
def preprocess_data(tokenizer, datasets):
    def tokenize_function(batch):
        return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=256)

    tokenized_datasets = datasets.map(tokenize_function, batched=True)
    return tokenized_datasets

# Compute Metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average="binary")
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

# Training pipeline
def train_and_evaluate_model():
    datasets = load_and_prepare_data()
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    tokenized_datasets = preprocess_data(tokenizer, datasets)

    # Load Pretrained Model
    model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

    # Training Arguments
    training_args = TrainingArguments(
        output_dir="./results",
        evaluation_strategy="steps",
        eval_steps=500,
        logging_steps=500,
        save_steps=500,
        num_train_epochs=2,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        save_total_limit=1,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["source_train"],
        eval_dataset=tokenized_datasets["target_train"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    # Train model
    trainer.train()

    # Evaluate on target domain
    print(trainer.evaluate())

# Run the pipeline
train_and_evaluate_model()

# Import libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import models
from transformers import BertTokenizer, BertModel
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Load Dataset
data = pd.read_csv("/content/tripadvisor_hotel_reviews.csv")
data['label'] = data['Rating'].apply(lambda x: 1 if x >= 4 else 0)  # Binarize labels

# Tokenizer and BERT Embeddings
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

# Dataset class
class TextDataset(Dataset):
    def __init__(self, texts, labels, max_len):
        self.texts = texts
        self.labels = labels
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')
        with torch.no_grad():
            embeddings = bert_model(**encoding).last_hidden_state.squeeze(0)  # (max_len, hidden_size)
        return embeddings, torch.tensor(label, dtype=torch.long)

# Prepare data
texts = data['Review'].tolist()
labels = data['label'].tolist()
train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)

# Hyperparameters
max_len = 128  # Maximum sequence length
batch_size = 16

# DataLoader
train_dataset = TextDataset(train_texts, train_labels, max_len)
val_dataset = TextDataset(val_texts, val_labels, max_len)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

# Adapt VGG16 for Text
class VGG16TextClassifier(nn.Module):
    def __init__(self, num_classes=2):
        super(VGG16TextClassifier, self).__init__()
        self.vgg16 = models.vgg16(pretrained=True)
        # Adjust the input layer to take embeddings as "images"
        self.vgg16.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        self.vgg16.classifier[6] = nn.Linear(4096, num_classes)  # Update the final layer

    def forward(self, x):
        x = x.unsqueeze(1)  # Add channel dimension for CNN
        x = self.vgg16(x)
        return x

# Initialize model, loss, and optimizer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = VGG16TextClassifier().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# Training loop
def train(model, train_loader, val_loader, criterion, optimizer, epochs=3):
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        correct = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()

        print(f"Epoch {epoch+1}/{epochs}, Loss: {train_loss/len(train_loader)}, Accuracy: {correct/len(train_loader.dataset)}")

# Run training
train(model, train_loader, val_loader, criterion, optimizer, epochs=3)

